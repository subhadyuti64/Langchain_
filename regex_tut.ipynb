{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03d9179b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "380282fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex is used to search for a specific pattern in a string\n",
    "chat1 = 'codebasics: you ask lot of questions ðŸ˜   1235678912, abc@xyz.com, 7889920091, (123)-567-8912,efgh@xyz.com'\n",
    "chat2 = 'codebasics: here it is: (123)-567-8912, abc@xyz.com'\n",
    "chat3 = 'codebasics: yes, phone: 1235678912 email: abc@xyz.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d55e6419",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\('\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\('\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\d'\n",
      "/var/folders/p_/x7_5dzhn12s3szm569470r9r0000gn/T/ipykernel_40527/2487002189.py:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  pattern1 = \"\\d{10}\"\n",
      "/var/folders/p_/x7_5dzhn12s3szm569470r9r0000gn/T/ipykernel_40527/2487002189.py:2: SyntaxWarning: invalid escape sequence '\\('\n",
      "  pattern2 = \"\\((\\d{3})\\)\"\n",
      "/var/folders/p_/x7_5dzhn12s3szm569470r9r0000gn/T/ipykernel_40527/2487002189.py:3: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  pattern3 = \"(\\d{10})|(\\(\\d{3}\\)-\\d{3}-\\d{4})\"\n"
     ]
    }
   ],
   "source": [
    "pattern1 = \"\\d{10}\"\n",
    "pattern2 = \"\\((\\d{3})\\)\"\n",
    "pattern3 = \"(\\d{10})|(\\(\\d{3}\\)-\\d{3}-\\d{4})\"\n",
    "pattern4 = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a3ec07b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '', '123']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(pattern4,chat1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a4877a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1235678912', ''), ('7889920091', ''), ('', '(123)-567-8912')]\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(pattern3,chat1))\n",
    "# print(re.findall(pattern3,chat2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "37539441",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\.'\n",
      "/var/folders/p_/x7_5dzhn12s3szm569470r9r0000gn/T/ipykernel_40527/2323558922.py:1: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  pattern5 = \"[a-z0-9A-Z]*@[a-z]*\\.[a-zA-Z]*\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['abc@xyz.com', 'efgh@xyz.com']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern5 = \"[a-z0-9A-Z]*@[a-z]*\\.[a-zA-Z]*\"\n",
    "\n",
    "re.findall(pattern5,chat1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bb3a479d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\d'\n",
      "/var/folders/p_/x7_5dzhn12s3szm569470r9r0000gn/T/ipykernel_40527/1973956005.py:5: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  pattern = 'order[^\\d]*(\\d*)'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'412889912'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat1='codebasics: Hello, I am having an issue with my order # 412889912'\n",
    "chat2='codebasics: I have a problem with my order number 412889912'\n",
    "chat3='codebasics: My order 412889912 is having an issue, I was charged 300$ when online it says 280$'\n",
    "\n",
    "pattern = 'order[^\\d]*(\\d*)'\n",
    "matches = re.findall(pattern, chat1)\n",
    "matches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7b76d51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I am Subhadyuti Rath. I am really ill and I want to book an appointment for my eye checkup with Dr.Ramesh Chandra. Can you please book an appointment for me with him on 22 June,2025 at 8pm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c3e9f4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(text):\n",
    "    patient_name = re.findall(r\"for (.*) with\", text)[0].strip()\n",
    "    doctor_name = re.findall(r\"with (.*) on\", text)[0].strip()\n",
    "    date = re.findall(r\"on (.*) at\", text)[0].strip()\n",
    "    time = re.findall(r\"at (.*)$\", text)[0].strip()\n",
    "    return {\n",
    "        \"Patient Name\": patient_name,\n",
    "        \"Doctor Name\": doctor_name,\n",
    "        \"Date\": date,\n",
    "        \"Time\": time\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5b764bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Patient Name': 'Mr. Subhadyuti Rath', 'Doctor Name': 'Dr.Ramesh Chandra', 'Date': '22 June,2025', 'Time': '10:00 AM'}\n"
     ]
    }
   ],
   "source": [
    "res = extract_data(text)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "da078f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1165f3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f2dfad9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template = \"\"\"Extract the following information from the following text: {text}\n",
    "    The informations are Patient Name, Doctor Name, Time of Appointment, Date of Appointment. Give the output in dictionary format\"\"\",\n",
    "    input_variables = [\"text\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9e3fbb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "db34c99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = chain.invoke({'text':text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4501cc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "    \"Patient Name\": \"Subhadyuti Rath\",\n",
      "    \"Doctor Name\": \"Dr.Ramesh Chandra\",\n",
      "    \"Time of Appointment\": \"8pm\",\n",
      "    \"Date of Appointment\": \"22 June,2025\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2b992922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.12/site-packages (4.52.4)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.33.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2025.4.26)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6c477cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2f01f5a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e266d2a236604e3b8f074a6a8994fff7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.61k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f81037bc95aa488a93f97bc26ee6387e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.26G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ImportError",
     "evalue": "FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Wav2Vec2Model\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m Wav2Vec2Model\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/wav2vec2-large-960h-lv60-self\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16, attn_implementation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attention_2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:309\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    311\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:4499\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4497\u001b[0m config \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(config)  \u001b[38;5;66;03m# We do not want to modify the config inplace in from_pretrained.\u001b[39;00m\n\u001b[1;32m   4498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_attn_implementation_autoset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m-> 4499\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_autoset_attn_implementation(\n\u001b[1;32m   4500\u001b[0m         config,\n\u001b[1;32m   4501\u001b[0m         use_flash_attention_2\u001b[38;5;241m=\u001b[39muse_flash_attention_2,\n\u001b[1;32m   4502\u001b[0m         torch_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype,\n\u001b[1;32m   4503\u001b[0m         device_map\u001b[38;5;241m=\u001b[39mdevice_map,\n\u001b[1;32m   4504\u001b[0m     )\n\u001b[1;32m   4506\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(model_init_context):\n\u001b[1;32m   4507\u001b[0m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[1;32m   4508\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(config, \u001b[38;5;241m*\u001b[39mmodel_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:2183\u001b[0m, in \u001b[0;36mPreTrainedModel._autoset_attn_implementation\u001b[0;34m(cls, config, use_flash_attention_2, torch_dtype, device_map, check_device_map)\u001b[0m\n\u001b[1;32m   2180\u001b[0m     config\u001b[38;5;241m.\u001b[39m_attn_implementation \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attention_2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39m_attn_implementation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attention_2\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 2183\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_check_and_enable_flash_attn_2(\n\u001b[1;32m   2184\u001b[0m         config,\n\u001b[1;32m   2185\u001b[0m         torch_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype,\n\u001b[1;32m   2186\u001b[0m         device_map\u001b[38;5;241m=\u001b[39mdevice_map,\n\u001b[1;32m   2187\u001b[0m         hard_check_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   2188\u001b[0m         check_device_map\u001b[38;5;241m=\u001b[39mcheck_device_map,\n\u001b[1;32m   2189\u001b[0m     )\n\u001b[1;32m   2190\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m requested_attn_implementation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflex_attention\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2191\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_check_and_enable_flex_attn(config, hard_check_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:2325\u001b[0m, in \u001b[0;36mPreTrainedModel._check_and_enable_flash_attn_2\u001b[0;34m(cls, config, torch_dtype, device_map, check_device_map, hard_check_only)\u001b[0m\n\u001b[1;32m   2323\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m config\n\u001b[1;32m   2324\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2325\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpreface\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m the package flash_attn seems to be not installed. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstall_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2327\u001b[0m flash_attention_version \u001b[38;5;241m=\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attn\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   2328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mcuda:\n",
      "\u001b[0;31mImportError\u001b[0m: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2."
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "418f89604a764792b926e87388d45129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.26G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Model\n",
    "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-large-960h-lv60-self\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\").to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
